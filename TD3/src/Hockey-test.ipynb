{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ocal/Desktop/RL-Hockey/env/lib/python3.11/site-packages/gymnasium/envs/registration.py:642: UserWarning: \u001b[33mWARN: Overriding environment Hockey-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "/home/ocal/Desktop/RL-Hockey/env/lib/python3.11/site-packages/gymnasium/envs/registration.py:642: UserWarning: \u001b[33mWARN: Overriding environment Hockey-One-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "/home/ocal/Desktop/RL-Hockey/TD3/td3.py:191: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.critic.load_state_dict(torch.load(filename + \"_critic.pth\"))\n",
      "/home/ocal/Desktop/RL-Hockey/TD3/td3.py:192: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.critic_optimizer.load_state_dict(torch.load(filename + \"_critic_optimizer.pth\"))\n",
      "/home/ocal/Desktop/RL-Hockey/TD3/td3.py:195: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.actor.load_state_dict(torch.load(filename + \"_actor.pth\"))\n",
      "/home/ocal/Desktop/RL-Hockey/TD3/td3.py:196: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.actor_optimizer.load_state_dict(torch.load(filename + \"_actor_optimizer.pth\"))\n",
      "/home/ocal/Desktop/RL-Hockey/TD3/td3.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.rnd.target_network.load_state_dict(torch.load(filename + \"_rnd_target.pth\"))\n",
      "/home/ocal/Desktop/RL-Hockey/TD3/td3.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.rnd.predictor_network.load_state_dict(torch.load(filename + \"_rnd_predictor.pth\"))\n",
      "/home/ocal/Desktop/RL-Hockey/TD3/td3.py:202: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.rnd.optimizer.load_state_dict(torch.load(filename + \"_rnd_optimizer.pth\"))\n",
      "Training mixed Seed 44:   0%|          | 12/70000 [00:02<4:43:55,  4.11it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 942\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;66;03m# Paths to the trained opponent agents\u001b[39;00m\n\u001b[1;32m    936\u001b[0m trained_opponent_paths \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    937\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels_hockey/vs_strong/seed_42/TD3_Hockey_vs_strong_seed_42_final.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    938\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels_hockey/mixed/seed_43/TD3_Hockey_mixed_seed_43_final.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    939\u001b[0m     \u001b[38;5;66;03m# Add more paths as needed\u001b[39;00m\n\u001b[1;32m    940\u001b[0m ]\n\u001b[0;32m--> 942\u001b[0m final_results \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepisodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_training_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_agent_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# or \"models_hockey/vs_strong/seed_42/TD3_Hockey_vs_strong_seed_42_final.pth\" to resume\u001b[39;49;00m\n\u001b[1;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopponent_agent_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrained_opponent_paths\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Provide the list of paths to the trained agents\u001b[39;49;00m\n\u001b[1;32m    950\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 723\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(mode, episodes, seed, save_model, training_config, load_agent_path, opponent_agent_paths)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;66;03m# Train the agent (after enough steps)\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_timesteps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m training_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_timesteps\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 723\u001b[0m     critic_loss, actor_loss \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    724\u001b[0m     cumulative_critic_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m critic_loss\n\u001b[1;32m    725\u001b[0m     loss_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/RL-Hockey/TD3/td3.py:139\u001b[0m, in \u001b[0;36mTD3.train\u001b[0;34m(self, replay_buffer, batch_size)\u001b[0m\n\u001b[1;32m    134\u001b[0m next_action \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_target(next_state) \u001b[38;5;241m+\u001b[39m noise\n\u001b[1;32m    136\u001b[0m )\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_action, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_action)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Compute the target Q value\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m target_Q1, target_Q2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m target_Q \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmin(target_Q1, target_Q2)\n\u001b[1;32m    141\u001b[0m target_Q \u001b[38;5;241m=\u001b[39m total_reward \u001b[38;5;241m+\u001b[39m not_done \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscount \u001b[38;5;241m*\u001b[39m target_Q\n",
      "File \u001b[0;32m~/Desktop/RL-Hockey/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/RL-Hockey/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/RL-Hockey/TD3/critic.py:59\u001b[0m, in \u001b[0;36mCritic.forward\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_layer_norm:\n\u001b[1;32m     58\u001b[0m     q2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln4(q2)\n\u001b[0;32m---> 59\u001b[0m q2 \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m q2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml5(q2)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_layer_norm:\n",
      "File \u001b[0;32m~/Desktop/RL-Hockey/env/lib/python3.11/site-packages/torch/nn/functional.py:1704\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1702\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1704\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1705\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "\n",
    "from tqdm import tqdm\n",
    "import gymnasium as gym\n",
    "\n",
    "# Import your TD3 components\n",
    "from TD3.memory import ReplayBuffer\n",
    "from TD3.td3 import TD3\n",
    "\n",
    "# Import the custom Hockey environment\n",
    "import hockey.hockey_env as h_env\n",
    "from importlib import reload\n",
    "\n",
    "# Reload the hockey environment to ensure the latest version is used\n",
    "reload(h_env)\n",
    "\n",
    "# ============================================\n",
    "# Environment Wrapper\n",
    "# ============================================\n",
    "class CustomHockeyEnv:\n",
    "    def __init__(self, mode=h_env.Mode.NORMAL, render_mode=None):\n",
    "        \"\"\"\n",
    "        Wraps the HockeyEnv to match the Gym API more closely.\n",
    "        \"\"\"\n",
    "        self.env = h_env.HockeyEnv(mode=mode)\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "    def reset(self):\n",
    "        obs, info = self.env.reset()\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        The custom step function returns (obs, r, d, t, info).\n",
    "        We convert to the standard Gym format: (obs, reward, done, info).\n",
    "        \"\"\"\n",
    "        obs, r, d, t, info = self.env.step(action)\n",
    "        done = d or t\n",
    "        return obs, r, done, info\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "    def seed(self, seed):\n",
    "        self.env.set_seed(seed)\n",
    "        self.env.action_space.seed(seed)\n",
    "\n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        return self.env.observation_space\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return self.env.action_space\n",
    "\n",
    "    def get_info_agent_two(self):\n",
    "        return self.env.get_info_agent_two()\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Trained Opponent Class\n",
    "# ============================================\n",
    "class TrainedOpponent:\n",
    "    def __init__(self, agent):\n",
    "        \"\"\"\n",
    "        Opponent that uses a trained TD3 agent to select actions.\n",
    "        \"\"\"\n",
    "        self.agent = agent\n",
    "\n",
    "    def act(self, observation):\n",
    "        \"\"\"\n",
    "        Selects an action based on the current observation using the trained agent.\n",
    "        \"\"\"\n",
    "        observation = torch.FloatTensor(observation).to(self.agent.device)\n",
    "        with torch.no_grad():\n",
    "            action = self.agent.actor(observation).cpu().numpy()\n",
    "        return action\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Opponent Definitions\n",
    "# ============================================\n",
    "def get_opponent(opponent_type, env, trained_agent=None):\n",
    "    \"\"\"\n",
    "    Returns an opponent based on the specified type ('weak', 'strong', 'trained').\n",
    "\n",
    "    Args:\n",
    "        opponent_type (str): Type of opponent ('weak', 'strong', 'trained').\n",
    "        env (CustomHockeyEnv): The environment instance.\n",
    "        trained_agent (TD3 or None): The trained agent to use as an opponent if opponent_type is 'trained'.\n",
    "\n",
    "    Returns:\n",
    "        An opponent instance compatible with the environment.\n",
    "    \"\"\"\n",
    "    if opponent_type == \"weak\":\n",
    "        return h_env.BasicOpponent(weak=True)\n",
    "    elif opponent_type == \"strong\":\n",
    "        return h_env.BasicOpponent(weak=False)\n",
    "    elif opponent_type == \"trained\":\n",
    "        if trained_agent is None:\n",
    "            raise ValueError(\"trained_agent must be provided for 'trained' opponent type.\")\n",
    "        return TrainedOpponent(trained_agent)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown opponent type: {opponent_type}\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Extended Evaluation Function\n",
    "# ============================================\n",
    "def eval_policy_extended(\n",
    "    policy, \n",
    "    eval_episodes=100, \n",
    "    seed=42,\n",
    "    mode=h_env.Mode.NORMAL,\n",
    "    opponent_type=None,\n",
    "    trained_agent=None  # Existing parameter\n",
    "):\n",
    "    \"\"\"\n",
    "    A single evaluation function that can handle:\n",
    "      - Normal mode with strong/weak/trained opponent\n",
    "      - Shooting mode (no opponent)\n",
    "      - Defense mode (no opponent)\n",
    "\n",
    "    If `opponent_type` is 'strong', 'weak', or 'trained', we create that opponent.\n",
    "    If `opponent_type` is None, we do not create any built-in opponent.\n",
    "\n",
    "    Returns a dictionary of evaluation stats:\n",
    "        {\n",
    "            \"avg_reward\": float,\n",
    "            \"win\": int,\n",
    "            \"loss\": int,\n",
    "            \"draw\": int,\n",
    "            \"win_rate\": float\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Create a new environment for evaluation to avoid state carry-over\n",
    "    eval_env = CustomHockeyEnv(mode=mode)\n",
    "    eval_env.seed(seed)\n",
    "\n",
    "    # Set networks to eval mode\n",
    "    policy.actor.eval()\n",
    "    policy.critic.eval()\n",
    "\n",
    "    # Initialize the opponent if needed\n",
    "    if opponent_type in [\"strong\", \"weak\", \"trained\"]:\n",
    "        opponent = get_opponent(opponent_type, eval_env, trained_agent=trained_agent)\n",
    "    else:\n",
    "        opponent = None  # e.g., for shooting or defense\n",
    "\n",
    "    total_rewards = []\n",
    "    results = {'win': 0, 'loss': 0, 'draw': 0}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(eval_episodes):\n",
    "            state, info = eval_env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                # Agent action\n",
    "                agent_action = policy.act(np.array(state), add_noise=False)\n",
    "\n",
    "                if opponent is not None:\n",
    "                    # Opponent action\n",
    "                    opponent_obs = eval_env.env.obs_agent_two()\n",
    "                    opponent_action = opponent.act(opponent_obs)\n",
    "                else:\n",
    "                    # No built-in opponent, so second agent does nothing\n",
    "                    opponent_action = np.array([0, 0, 0, 0], dtype=np.float32)\n",
    "\n",
    "                # Combine actions\n",
    "                full_action = np.hstack([agent_action, opponent_action])\n",
    "\n",
    "                next_state, reward, done, info = eval_env.step(full_action)\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "\n",
    "            total_rewards.append(episode_reward)\n",
    "\n",
    "            # Determine final outcome via `env._get_info()`\n",
    "            final_info = eval_env.env._get_info()\n",
    "            winner = final_info.get('winner', 0)  # 1 => agent1, -1 => agent1 loses, 0 => draw\n",
    "\n",
    "            if winner == 1:\n",
    "                results['win'] += 1\n",
    "            elif winner == -1:\n",
    "                results['loss'] += 1\n",
    "            else:\n",
    "                results['draw'] += 1\n",
    "\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    total_games = results['win'] + results['loss'] + results['draw']\n",
    "    win_rate = (results['win'] / total_games) if total_games > 0 else 0.0\n",
    "\n",
    "    # Restore networks to train mode\n",
    "    policy.actor.train()\n",
    "    policy.critic.train()\n",
    "\n",
    "    eval_stats = {\n",
    "        \"avg_reward\": float(avg_reward),\n",
    "        \"win\": results['win'],\n",
    "        \"loss\": results['loss'],\n",
    "        \"draw\": results['draw'],\n",
    "        \"win_rate\": float(win_rate)\n",
    "    }\n",
    "\n",
    "    return eval_stats\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Plotting Functions\n",
    "# ============================================\n",
    "def plot_losses(loss_data, mode, save_path):\n",
    "    episodes = range(1, len(loss_data[\"critic_loss\"]) + 1)\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    # Plot Critic Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(episodes, loss_data[\"critic_loss\"], label='Critic Loss', color='blue')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Critic Loss over Episodes for {mode}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot Actor Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(episodes, loss_data[\"actor_loss\"], label='Actor Loss', color='orange')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Actor Loss over Episodes for {mode}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_path, f\"losses_{mode}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "def plot_overall_statistics(overall_stats, opponent_type, save_path):\n",
    "    \"\"\"\n",
    "    Plots overall statistics for an opponent type.\n",
    "\n",
    "    Args:\n",
    "        overall_stats (dict): Statistics including win_rate, loss_rate, etc.\n",
    "        opponent_type (str): Type of opponent.\n",
    "        save_path (str): Directory to save the plots.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    categories = ['Win Rate', 'Loss Rate', 'Draw Rate']\n",
    "    values = [overall_stats['Agent1']['win_rate'], \n",
    "              overall_stats['Agent1']['loss_rate'], \n",
    "              overall_stats['Agent1']['draw_rate']]\n",
    "    colors = ['green', 'red', 'blue']\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.bar(categories, values, color=colors)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_ylabel('Rate')\n",
    "    ax.set_title(f'Overall Statistics vs {opponent_type.capitalize()} Opponent')\n",
    "\n",
    "    for i, v in enumerate(values):\n",
    "        ax.text(i, v + 0.02, f\"{v:.2f}\", ha='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_path, f\"overall_statistics_{opponent_type}.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "def plot_rewards(rewards, mode, save_path, window=20):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(rewards, label='Episode Reward', alpha=0.3)\n",
    "    if len(rewards) >= window:\n",
    "        moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "        plt.plot(range(window-1, len(rewards)), moving_avg, label=f'{window}-Episode Moving Average', color='red')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title(f'TD3 on {mode}: Episode Rewards with Moving Average')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(save_path, f\"rewards_{mode}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_multi_winrate_curves(winrate_dict, mode, save_path):\n",
    "    \"\"\"\n",
    "    Plots multiple win-rate curves in the same figure.\n",
    "    `winrate_dict` should be a dictionary like:\n",
    "       {\n",
    "         \"strong\": [...],\n",
    "         \"weak\": [...],\n",
    "         \"trained_1\": [...],\n",
    "         \"trained_2\": [...],\n",
    "         \"shooting\": [...],\n",
    "         \"defense\": [...]\n",
    "       }\n",
    "    Each list is the time series of the agent's win rate in that scenario.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12,6))\n",
    "\n",
    "    # We'll plot each scenario if it exists\n",
    "    for scenario, wr_list in winrate_dict.items():\n",
    "        if len(wr_list) > 0:\n",
    "            plt.plot(wr_list, label=f'Win Rate vs {scenario.capitalize()}')\n",
    "\n",
    "    plt.xlabel('Evaluation Index (every eval_freq episodes)')\n",
    "    plt.ylabel('Win Rate')\n",
    "    plt.title(f'{mode} - Win Rates Over Training (All Scenarios)')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(save_path, f\"winrates_{mode}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_noise_comparison(agent, training_config, env_name, save_path):\n",
    "    \"\"\"\n",
    "    Plots Pink/OU noise vs Gaussian noise for comparison, if the agent uses pink or OU noise.\n",
    "    \"\"\"\n",
    "    if training_config[\"expl_noise_type\"].lower() not in [\"pink\", \"ou\"]:\n",
    "        print(f\"No noise comparison plot available for noise type: {training_config['expl_noise_type']}\")\n",
    "        return\n",
    "\n",
    "    max_steps = training_config.get(\"max_episode_steps\", 600)\n",
    "    action_dim = agent.action_dim\n",
    "\n",
    "    # Generate noise sequence\n",
    "    noise_sequence = []\n",
    "    for _ in range(max_steps):\n",
    "        if training_config[\"expl_noise_type\"].lower() == \"pink\":\n",
    "            noise = agent.pink_noise.get_noise() * training_config[\"expl_noise\"]\n",
    "        elif training_config[\"expl_noise_type\"].lower() == \"ou\":\n",
    "            noise = agent.ou_noise.sample() * training_config[\"expl_noise\"]\n",
    "        noise_sequence.append(noise)\n",
    "    noise_sequence = np.array(noise_sequence)\n",
    "\n",
    "    # Generate Gaussian noise for comparison\n",
    "    gaussian_noise_sequence = np.random.normal(0, training_config[\"expl_noise\"], size=(max_steps, action_dim))\n",
    "\n",
    "    # Plot for each action dimension\n",
    "    for dim in range(action_dim):\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.plot(noise_sequence[:, dim], label=f'{training_config[\"expl_noise_type\"].capitalize()} Noise')\n",
    "        plt.plot(gaussian_noise_sequence[:, dim], label='Gaussian Noise', alpha=0.7)\n",
    "        plt.title(f'Noise Comparison for Action Dimension {dim} in {env_name}')\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Noise Value')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(os.path.join(save_path, f\"noise_comparison_dim_{dim}_{env_name}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Logging Setup\n",
    "# ============================================\n",
    "def setup_evaluation_logging(results_dir, seed, opponent_type):\n",
    "    log_file = os.path.join(results_dir, f\"evaluation_log_seed_{seed}_{opponent_type}.log\")\n",
    "    # To prevent adding multiple handlers if this function is called multiple times\n",
    "    if not logging.getLogger().hasHandlers():\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s | %(levelname)s | %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(log_file),\n",
    "                logging.StreamHandler()  # Also log to console\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        # Add a new handler for each opponent_type\n",
    "        logger = logging.getLogger()\n",
    "        handler = logging.FileHandler(log_file)\n",
    "        formatter = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "    logging.info(f\"Evaluation started for opponent_type: {opponent_type}, seed: {seed}\")\n",
    "\n",
    "\n",
    "def setup_logging(results_dir, seed, mode):\n",
    "    log_file = os.path.join(results_dir, f\"training_log_seed_{seed}.log\")\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s | %(levelname)s | %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file)\n",
    "        ]\n",
    "    )\n",
    "    logging.info(f\"Training started with mode: {mode}, seed: {seed}\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Save Training Information\n",
    "# ============================================\n",
    "def save_training_info(config, mode, mixed_cycle, opponent_agent_paths, results_dir):\n",
    "    # Convert mixed_cycle to a serializable format (convert Mode enums to strings)\n",
    "    if mixed_cycle is not None:\n",
    "        mixed_cycle_serializable = []\n",
    "        for opponent, mode_enum in mixed_cycle:\n",
    "            mode_str = mode_enum.name if mode_enum else None\n",
    "            mixed_cycle_serializable.append((opponent, mode_str))\n",
    "    else:\n",
    "        mixed_cycle_serializable = None\n",
    "\n",
    "    training_info = {\n",
    "        \"training_config\": config,\n",
    "        \"training_mode\": mode,\n",
    "        \"mixed_cycle\": mixed_cycle_serializable,\n",
    "        \"opponent_agent_paths\": opponent_agent_paths\n",
    "    }\n",
    "    with open(os.path.join(results_dir, \"training_info.json\"), \"w\") as f:\n",
    "        json.dump(training_info, f, indent=4)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Training Loop\n",
    "# ============================================\n",
    "def get_trained_opponent_index(episode, num_trained_opponents):\n",
    "    \"\"\"\n",
    "    Determines which trained opponent to use based on the episode number.\n",
    "    Cycles through the available trained opponents.\n",
    "    \"\"\"\n",
    "    if num_trained_opponents == 0:\n",
    "        return None\n",
    "    return (episode - 1) % num_trained_opponents\n",
    "\n",
    "\n",
    "def main(\n",
    "    mode=\"vs_strong\",\n",
    "    episodes=700,\n",
    "    seed=42,\n",
    "    save_model=True,\n",
    "    training_config=None,\n",
    "    load_agent_path=None,\n",
    "    opponent_agent_paths=None  # Changed from single path to list of paths\n",
    "):\n",
    "    \"\"\"\n",
    "    Main function to train TD3 on the Custom Hockey Environment in various modes:\n",
    "      - \"vs_strong\": Always train against the strong built-in opponent.\n",
    "      - \"vs_weak\": Always train against the weak built-in opponent.\n",
    "      - \"shooting\": Train in shooting mode (no built-in opponent needed).\n",
    "      - \"defense\": Train in defense mode (no built-in opponent needed).\n",
    "      - \"mixed\": Cycle between (strong, weak, trained1, trained2, ..., shooting, defense) each episode.\n",
    "\n",
    "    Args:\n",
    "        mode (str): Training mode (\"vs_strong\", \"vs_weak\", \"shooting\", \"defense\", \"mixed\").\n",
    "        episodes (int): Number of training episodes.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "        save_model (bool): Whether to save the trained models.\n",
    "        training_config (dict): Dictionary containing training configurations.\n",
    "        load_agent_path (str or None): If provided, loads an existing agent from this path before training.\n",
    "        opponent_agent_paths (list of str or None): List of paths to trained agents to be used as opponents in mixed mode.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of final evaluation results.\n",
    "    \"\"\"\n",
    "    # Default training configuration if none provided\n",
    "    if training_config is None:\n",
    "        training_config = {\n",
    "            \"discount\": 0.99,\n",
    "            \"tau\": 0.005,\n",
    "            \"policy_noise\": 0.2,\n",
    "            \"noise_clip\": 0.5,\n",
    "            \"policy_freq\": 2,\n",
    "            \"max_episodes\": episodes,       # total training episodes\n",
    "            \"start_timesteps\": 1000,        # number of initial random steps\n",
    "            \"eval_freq\": 50,                # how often to evaluate (in episodes)\n",
    "            \"batch_size\": 256,              # batch size for training\n",
    "            \"expl_noise_type\": \"pink\",      # type of exploration noise: \"gaussian\", \"pink\", or \"ou\"\n",
    "            \"expl_noise\": 0.1,              # exploration noise scale\n",
    "            \"pink_noise_params\": {          # pink noise specific params\n",
    "                \"exponent\": 1.0,\n",
    "                \"fmin\": 0.0\n",
    "            },\n",
    "            \"ou_noise_params\": {            # OU noise specific params\n",
    "                \"mu\": 0.0,\n",
    "                \"theta\": 0.15,\n",
    "                \"sigma\": 0.2\n",
    "            },\n",
    "            \"use_layer_norm\": True,         # toggle LayerNorm\n",
    "            \"ln_eps\": 1e-5,                \n",
    "            \"save_model\": save_model,\n",
    "            \"save_model_freq\": 100,         # how often to save\n",
    "            \"use_rnd\": True,                # toggle RND\n",
    "            \"rnd_weight\": 1.0,\n",
    "            \"rnd_lr\": 1e-4,\n",
    "            \"rnd_hidden_dim\": 128,\n",
    "            \"max_episode_steps\": 600\n",
    "        }\n",
    "\n",
    "    # Decide environment mode based on user choice\n",
    "    # If user chooses \"shooting\" or \"defense\", we set env_mode accordingly. \n",
    "    # If user chooses \"mixed\", we will override the env mode each episode.\n",
    "    if mode == \"shooting\":\n",
    "        env_mode = h_env.Mode.TRAIN_SHOOTING\n",
    "    elif mode == \"defense\":\n",
    "        env_mode = h_env.Mode.TRAIN_DEFENSE\n",
    "    else:\n",
    "        # For \"vs_strong\", \"vs_weak\", or \"mixed\", use NORMAL\n",
    "        env_mode = h_env.Mode.NORMAL\n",
    "\n",
    "    # Create environment\n",
    "    env = CustomHockeyEnv(mode=env_mode)\n",
    "\n",
    "    # Set seeds\n",
    "    env.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Environment dimensions\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = 4  # We only control the first 4 actions for the first agent\n",
    "    max_action = float(env.action_space.high[0])\n",
    "\n",
    "    # Directories\n",
    "    base_results_dir = \"./results_hockey\"\n",
    "    base_models_dir = \"./models_hockey\"\n",
    "    os.makedirs(base_results_dir, exist_ok=True)\n",
    "    os.makedirs(base_models_dir, exist_ok=True)\n",
    "\n",
    "    file_name = f\"TD3_Hockey_{mode}_seed_{seed}\"\n",
    "    results_dir = os.path.join(base_results_dir, mode, f\"seed_{seed}\")\n",
    "    models_dir = os.path.join(base_models_dir, mode, f\"seed_{seed}\")\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    if save_model:\n",
    "        os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "    # Setup logging\n",
    "    setup_logging(results_dir, seed, mode)\n",
    "\n",
    "    logging.info(f\"Training Mode: {mode}\")\n",
    "    logging.info(f\"Environment Mode: {env_mode}\")\n",
    "    logging.info(f\"State Dim: {state_dim}, Action Dim: {action_dim}, Max Action: {max_action}\")\n",
    "\n",
    "    # Save training info\n",
    "    if mode == \"mixed\":\n",
    "        mixed_cycle = [\n",
    "            (\"strong\", h_env.Mode.NORMAL),\n",
    "            (\"strong\", h_env.Mode.NORMAL),\n",
    "            (\"strong\", h_env.Mode.NORMAL),\n",
    "            (\"weak\", h_env.Mode.NORMAL),\n",
    "            (\"trained\", h_env.Mode.NORMAL),\n",
    "            (None,     h_env.Mode.TRAIN_SHOOTING),\n",
    "            (None,     h_env.Mode.TRAIN_DEFENSE)\n",
    "        ]\n",
    "    else:\n",
    "        mixed_cycle = None\n",
    "\n",
    "    save_training_info(training_config, mode, mixed_cycle, opponent_agent_paths, results_dir)\n",
    "\n",
    "    # Initialize the TD3 agent\n",
    "    agent = TD3(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        max_action=max_action,\n",
    "        training_config=training_config,\n",
    "    )\n",
    "\n",
    "    # If we have a path to load from, do so\n",
    "    if load_agent_path is not None:\n",
    "        logging.info(f\"Loading agent from: {load_agent_path}\")\n",
    "        agent.load(load_agent_path)\n",
    "\n",
    "    # Load the opponent agents if provided and mode is mixed\n",
    "    opponent_agents = []\n",
    "    if mode == \"mixed\" and opponent_agent_paths is not None:\n",
    "        for idx, path in enumerate(opponent_agent_paths):\n",
    "            logging.info(f\"Loading opponent agent {idx + 1} from: {path}\")\n",
    "            trained_agent = TD3(\n",
    "                state_dim=state_dim,\n",
    "                action_dim=action_dim,\n",
    "                max_action=max_action,\n",
    "                training_config=training_config,\n",
    "            )\n",
    "            trained_agent.load(path)\n",
    "            trained_agent.critic.eval()\n",
    "            trained_agent.actor.eval()\n",
    "            trained_agent.actor_target.eval()\n",
    "            trained_agent.critic_target.eval()\n",
    "            opponent_agents.append(trained_agent)\n",
    "\n",
    "    # Replay buffer\n",
    "    replay_buffer = ReplayBuffer(state_dim, action_dim, max_size=int(1e6))\n",
    "\n",
    "    # Tracking\n",
    "    total_timesteps = 0\n",
    "    evaluation_results = []   # store training rewards (per-episode)\n",
    "    loss_results = {\n",
    "        \"critic_loss\": [],\n",
    "        \"actor_loss\": []\n",
    "    }\n",
    "\n",
    "    # We track multiple win-rate curves for each scenario:\n",
    "    # 1. vs Strong\n",
    "    # 2. vs Weak\n",
    "    # 3. vs Trained1, vs Trained2, etc.\n",
    "    # 4. Shooting\n",
    "    # 5. Defense\n",
    "    evaluation_winrates = {\n",
    "        \"strong\": [],\n",
    "        \"weak\": [],\n",
    "        # Dynamically add \"trained_1\", \"trained_2\", etc.\n",
    "        \"shooting\": [],\n",
    "        \"defense\": []\n",
    "    }\n",
    "\n",
    "    # Opponent function for the \"mixed\" mode\n",
    "    # We'll cycle through different (opponent, env_mode)\n",
    "    if mode == \"mixed\":\n",
    "        mixed_cycle = [\n",
    "            (\"strong\", h_env.Mode.NORMAL),\n",
    "            (\"strong\", h_env.Mode.NORMAL),\n",
    "            (\"strong\", h_env.Mode.NORMAL),\n",
    "            (\"weak\", h_env.Mode.NORMAL),\n",
    "            (\"trained\", h_env.Mode.NORMAL),\n",
    "            (None,     h_env.Mode.TRAIN_SHOOTING),\n",
    "            (None,     h_env.Mode.TRAIN_DEFENSE)\n",
    "        ]\n",
    "    else:\n",
    "        mixed_cycle = None  # Not used for other modes\n",
    "\n",
    "    pbar = tqdm(total=training_config[\"max_episodes\"], desc=f\"Training {mode} Seed {seed}\")\n",
    "    for episode in range(1, training_config[\"max_episodes\"] + 1):\n",
    "        # Decide opponent and environment mode for this episode\n",
    "        if mode == \"vs_strong\":\n",
    "            opponent_type = \"strong\"\n",
    "            current_env_mode = env_mode\n",
    "            selected_trained_agent = None\n",
    "        elif mode == \"vs_weak\":\n",
    "            opponent_type = \"weak\"\n",
    "            current_env_mode = env_mode\n",
    "            selected_trained_agent = None\n",
    "        elif mode == \"shooting\":\n",
    "            opponent_type = None\n",
    "            current_env_mode = h_env.Mode.TRAIN_SHOOTING\n",
    "            selected_trained_agent = None\n",
    "        elif mode == \"defense\":\n",
    "            opponent_type = None\n",
    "            current_env_mode = h_env.Mode.TRAIN_DEFENSE\n",
    "            selected_trained_agent = None\n",
    "        elif mode == \"mixed\":\n",
    "            # Cycle through different (opponent, env_mode)\n",
    "            idx = (episode - 1) % len(mixed_cycle)\n",
    "            cycle_opponent_type, cycle_env_mode = mixed_cycle[idx]\n",
    "            \n",
    "            if cycle_opponent_type == \"trained\" and len(opponent_agents) > 0:\n",
    "                # Select which trained opponent to use\n",
    "                trained_idx = get_trained_opponent_index(episode, len(opponent_agents))\n",
    "                opponent_type = \"trained\"\n",
    "                current_env_mode = cycle_env_mode\n",
    "                selected_trained_agent = opponent_agents[trained_idx]\n",
    "            else:\n",
    "                opponent_type = cycle_opponent_type\n",
    "                current_env_mode = cycle_env_mode\n",
    "                selected_trained_agent = None\n",
    "\n",
    "        # If needed, reinitialize environment for changed mode\n",
    "        if mode == \"mixed\" and env.env.mode != current_env_mode:\n",
    "            env.close()\n",
    "            env = CustomHockeyEnv(mode=current_env_mode)\n",
    "            env.seed(seed + episode)  # or some offset\n",
    "            logging.info(f\"Switched environment mode to: {current_env_mode}\")\n",
    "\n",
    "        # Create opponent if applicable\n",
    "        if opponent_type in [\"strong\", \"weak\", \"trained\"]:\n",
    "            opponent = get_opponent(opponent_type, env, trained_agent=selected_trained_agent)\n",
    "        else:\n",
    "            opponent = None  # no built-in opponent for shooting/defense\n",
    "\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        episode_timesteps = 0\n",
    "\n",
    "        # Reset noise if needed (OU or Pink)\n",
    "        if (training_config[\"expl_noise_type\"].lower() == \"ou\") and hasattr(agent, 'ou_noise'):\n",
    "            agent.ou_noise.reset()\n",
    "        if (training_config[\"expl_noise_type\"].lower() == \"pink\") and hasattr(agent, 'pink_noise'):\n",
    "            agent.pink_noise.reset()\n",
    "\n",
    "        # Reset loss accumulators\n",
    "        cumulative_critic_loss = 0.0\n",
    "        cumulative_actor_loss = 0.0\n",
    "        loss_steps = 0\n",
    "        actor_loss_steps = 0\n",
    "\n",
    "        while not done:\n",
    "            episode_timesteps += 1\n",
    "            total_timesteps += 1\n",
    "\n",
    "            # Select action\n",
    "            if total_timesteps < training_config[\"start_timesteps\"]:\n",
    "                action = env.env.action_space.sample()[:action_dim]\n",
    "            else:\n",
    "                action = agent.act(np.array(state), add_noise=True)\n",
    "\n",
    "            # Opponent action if applicable\n",
    "            if opponent is not None:\n",
    "                opp_obs = env.env.obs_agent_two()\n",
    "                opp_action = opponent.act(opp_obs)\n",
    "                full_action = np.hstack([action, opp_action])\n",
    "            else:\n",
    "                # Shooting/Defense modes do not require an explicit opponent\n",
    "                full_action = np.hstack([action, [0,0,0,0]])\n",
    "\n",
    "            next_state, reward, done, info = env.step(full_action)\n",
    "            done_bool = float(done) if episode_timesteps < training_config[\"max_episode_steps\"] else 0\n",
    "\n",
    "            # Store in replay buffer\n",
    "            replay_buffer.add(state, action, next_state, reward, done_bool)\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Train the agent (after enough steps)\n",
    "            if total_timesteps >= training_config[\"start_timesteps\"]:\n",
    "                critic_loss, actor_loss = agent.train(replay_buffer, training_config[\"batch_size\"])\n",
    "                cumulative_critic_loss += critic_loss\n",
    "                loss_steps += 1\n",
    "                if actor_loss is not None:\n",
    "                    cumulative_actor_loss += actor_loss\n",
    "                    actor_loss_steps += 1\n",
    "\n",
    "        # Track losses\n",
    "        avg_critic_loss = cumulative_critic_loss / loss_steps if loss_steps > 0 else 0\n",
    "        avg_actor_loss = cumulative_actor_loss / actor_loss_steps if actor_loss_steps > 0 else 0\n",
    "        loss_results[\"critic_loss\"].append(avg_critic_loss)\n",
    "        loss_results[\"actor_loss\"].append(avg_actor_loss)\n",
    "\n",
    "        # Track rewards\n",
    "        evaluation_results.append(episode_reward)\n",
    "\n",
    "        # Log per-episode information\n",
    "        logging.info(f\"Episode {episode} | Reward: {episode_reward:.2f} | Critic Loss: {avg_critic_loss:.4f} | Actor Loss: {avg_actor_loss:.4f}\")\n",
    "\n",
    "        # Evaluate at intervals\n",
    "        if episode % training_config[\"eval_freq\"] == 0:\n",
    "            logging.info(f\"===== Evaluation at Episode {episode} =====\")\n",
    "            # Evaluate vs STRONG\n",
    "            stats_strong = eval_policy_extended(\n",
    "                policy=agent, \n",
    "                eval_episodes=100,\n",
    "                seed=seed + 10, \n",
    "                mode=h_env.Mode.NORMAL,\n",
    "                opponent_type=\"strong\"\n",
    "            )\n",
    "            evaluation_winrates[\"strong\"].append(stats_strong[\"win_rate\"])\n",
    "            logging.info(f\"  vs Strong  => WinRate: {stats_strong['win_rate']:.2f}\")\n",
    "\n",
    "            # Evaluate vs WEAK\n",
    "            stats_weak = eval_policy_extended(\n",
    "                policy=agent, \n",
    "                eval_episodes=100,\n",
    "                seed=seed + 20, \n",
    "                mode=h_env.Mode.NORMAL,\n",
    "                opponent_type=\"weak\"\n",
    "            )\n",
    "            evaluation_winrates[\"weak\"].append(stats_weak[\"win_rate\"])\n",
    "            logging.info(f\"  vs Weak    => WinRate: {stats_weak['win_rate']:.2f}\")\n",
    "\n",
    "            # Evaluate each TRAINED opponent\n",
    "            if mode == \"mixed\" and len(opponent_agents) > 0:\n",
    "                for idx, trained_agent in enumerate(opponent_agents):\n",
    "                    stats_trained = eval_policy_extended(\n",
    "                        policy=agent, \n",
    "                        eval_episodes=100,\n",
    "                        seed=seed + 25 + idx,  # Different seed for each opponent\n",
    "                        mode=h_env.Mode.NORMAL,\n",
    "                        opponent_type=\"trained\",\n",
    "                        trained_agent=trained_agent\n",
    "                    )\n",
    "                    key = f\"trained_{idx + 1}\"\n",
    "                    evaluation_winrates[key] = evaluation_winrates.get(key, [])\n",
    "                    evaluation_winrates[key].append(stats_trained[\"win_rate\"])\n",
    "                    logging.info(f\"  vs Trained_{idx + 1} => WinRate: {stats_trained['win_rate']:.2f}\")\n",
    "\n",
    "            # Evaluate SHOOTING mode\n",
    "            stats_shooting = eval_policy_extended(\n",
    "                policy=agent, \n",
    "                eval_episodes=100,\n",
    "                seed=seed + 30, \n",
    "                mode=h_env.Mode.TRAIN_SHOOTING,\n",
    "                opponent_type=None\n",
    "            )\n",
    "            evaluation_winrates[\"shooting\"].append(stats_shooting[\"win_rate\"])\n",
    "            logging.info(f\"  Shooting   => WinRate: {stats_shooting['win_rate']:.2f}\")\n",
    "\n",
    "            # Evaluate DEFENSE mode\n",
    "            stats_defense = eval_policy_extended(\n",
    "                policy=agent, \n",
    "                eval_episodes=100,\n",
    "                seed=seed + 40, \n",
    "                mode=h_env.Mode.TRAIN_DEFENSE,\n",
    "                opponent_type=None\n",
    "            )\n",
    "            evaluation_winrates[\"defense\"].append(stats_defense[\"win_rate\"])\n",
    "            logging.info(f\"  Defense    => WinRate: {stats_defense['win_rate']:.2f}\")\n",
    "\n",
    "            # Save intermediate results\n",
    "            np.save(os.path.join(results_dir, f\"{file_name}_evaluations.npy\"), evaluation_results)\n",
    "            np.save(os.path.join(results_dir, f\"{file_name}_winrates.npy\"), evaluation_winrates)\n",
    "\n",
    "            # Optionally save model\n",
    "            if save_model and (episode % training_config[\"save_model_freq\"] == 0):\n",
    "                agent.save(os.path.join(models_dir, f\"{file_name}_episode_{episode}.pth\"))\n",
    "                logging.info(f\"Saved model at episode {episode}\")\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    # Optionally save final model\n",
    "    if save_model:\n",
    "        agent.save(os.path.join(models_dir, f\"{file_name}_final.pth\"))\n",
    "        logging.info(f\"Saved final model to {models_dir}\")\n",
    "\n",
    "    # ==========================\n",
    "    # Post-Training Analysis\n",
    "    # ==========================\n",
    "    evaluation_results = np.array(evaluation_results)\n",
    "\n",
    "    # Plot the loss curves\n",
    "    plot_losses(loss_results, mode, results_dir)\n",
    "\n",
    "    # Plot the episode rewards\n",
    "    plot_rewards(evaluation_results, mode, results_dir, window=20)\n",
    "\n",
    "    # Plot the stored multi-scenario win-rate curves (across training)\n",
    "    if len(evaluation_winrates[\"strong\"]) > 0:\n",
    "        plot_multi_winrate_curves(evaluation_winrates, mode, results_dir)\n",
    "\n",
    "    # Plot the final noise comparison (if OU or Pink)\n",
    "    plot_noise_comparison(agent, training_config, mode, results_dir)\n",
    "\n",
    "    # -- Final Extended Evaluation (again, for all scenarios) --\n",
    "    logging.info(\"\\n===== Final Extended Evaluation (100 episodes each scenario) =====\")\n",
    "    final_stats_strong = eval_policy_extended(agent, mode=h_env.Mode.NORMAL, opponent_type=\"strong\", seed=seed+100)\n",
    "    final_stats_weak = eval_policy_extended(agent, mode=h_env.Mode.NORMAL, opponent_type=\"weak\", seed=seed+200)\n",
    "    if mode == \"mixed\" and len(opponent_agents) > 0:\n",
    "        for idx, trained_agent in enumerate(opponent_agents):\n",
    "            final_stats_trained = eval_policy_extended(\n",
    "                agent, \n",
    "                mode=h_env.Mode.NORMAL, \n",
    "                opponent_type=\"trained\", \n",
    "                seed=seed+250+idx, \n",
    "                trained_agent=trained_agent\n",
    "            )\n",
    "            logging.info(f\"  vs Trained_{idx + 1} => WinRate: {final_stats_trained['win_rate']:.2f}\")\n",
    "    final_stats_shooting = eval_policy_extended(agent, mode=h_env.Mode.TRAIN_SHOOTING, opponent_type=None, seed=seed+300)\n",
    "    final_stats_defense = eval_policy_extended(agent, mode=h_env.Mode.TRAIN_DEFENSE, opponent_type=None, seed=seed+400)\n",
    "\n",
    "    logging.info(f\"  vs Strong  => WinRate: {final_stats_strong['win_rate']:.2f}\")\n",
    "    logging.info(f\"  vs Weak    => WinRate: {final_stats_weak['win_rate']:.2f}\")\n",
    "    logging.info(f\"  Shooting   => WinRate: {final_stats_shooting['win_rate']:.2f}\")\n",
    "    logging.info(f\"  Defense    => WinRate: {final_stats_defense['win_rate']:.2f}\")\n",
    "\n",
    "    # Store final results in the dictionary\n",
    "    final_evaluation_results = {\n",
    "        \"final_eval_strong\": final_stats_strong,\n",
    "        \"final_eval_weak\": final_stats_weak,\n",
    "        \"final_eval_shooting\": final_stats_shooting,\n",
    "        \"final_eval_defense\": final_stats_defense,\n",
    "    }\n",
    "\n",
    "    if mode == \"mixed\" and len(opponent_agents) > 0:\n",
    "        for idx, trained_agent in enumerate(opponent_agents):\n",
    "            # Ensure the final_stats_trained is correctly stored\n",
    "            key = f\"final_eval_trained_{idx + 1}\"\n",
    "            final_evaluation_results[key] = eval_policy_extended(\n",
    "                agent, \n",
    "                mode=h_env.Mode.NORMAL, \n",
    "                opponent_type=\"trained\", \n",
    "                seed=seed+250+idx, \n",
    "                trained_agent=trained_agent\n",
    "            )\n",
    "\n",
    "    final_evaluation_results.update({\n",
    "        \"loss_results\": loss_results,\n",
    "        \"training_rewards\": evaluation_results.tolist(),\n",
    "        \"winrates_vs_scenarios\": evaluation_winrates\n",
    "    })\n",
    "\n",
    "    # Plot the stored multi-scenario win-rate curves (across training)\n",
    "    if len(evaluation_winrates[\"strong\"]) > 0:\n",
    "        plot_multi_winrate_curves(evaluation_winrates, mode, results_dir)\n",
    "\n",
    "    # Save final evaluation dictionary\n",
    "    with open(os.path.join(results_dir, f\"{file_name}_final_evaluations.json\"), \"w\") as f:\n",
    "        json.dump(final_evaluation_results, f, indent=4)\n",
    "\n",
    "    logging.info(\"Training completed successfully.\")\n",
    "    return final_evaluation_results\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Example Usage\n",
    "# ===============================\n",
    "if __name__ == \"__main__\":\n",
    "    mode = \"mixed\"  # options: \"vs_weak\", \"vs_strong\", \"shooting\", \"defense\", \"mixed\"\n",
    "    episodes = 70000\n",
    "    seed = 44\n",
    "    save_model = True\n",
    "\n",
    "    custom_training_config = {\n",
    "        \"discount\": 0.99,\n",
    "        \"tau\": 0.005,\n",
    "        \"policy_noise\": 0.2,\n",
    "        \"noise_clip\": 0.5,\n",
    "        \"policy_freq\": 2,\n",
    "        \"max_episodes\": episodes,\n",
    "        \"start_timesteps\": 1000,\n",
    "        \"eval_freq\": 200,      # Evaluate every 200 episodes\n",
    "        \"batch_size\": 256,\n",
    "        \"expl_noise_type\": \"pink\",\n",
    "        \"expl_noise\": 0.1,\n",
    "        \"pink_noise_params\": {\"exponent\": 1.0, \"fmin\": 0.0},\n",
    "        \"ou_noise_params\": {\"mu\": 0.0, \"theta\": 0.15, \"sigma\": 0.2},\n",
    "        \"use_layer_norm\": True,\n",
    "        \"ln_eps\": 1e-5,\n",
    "        \"save_model\": save_model,\n",
    "        \"save_model_freq\": 10000,\n",
    "        \"use_rnd\": True,\n",
    "        \"rnd_weight\": 1.0,\n",
    "        \"rnd_lr\": 1e-4,\n",
    "        \"rnd_hidden_dim\": 128,\n",
    "        \"max_episode_steps\": 600\n",
    "    }\n",
    "\n",
    "    # Paths to the trained opponent agents\n",
    "    trained_opponent_paths = [\n",
    "        \"models_hockey/vs_strong/seed_42/TD3_Hockey_vs_strong_seed_42_final.pth\",\n",
    "        \"models_hockey/mixed/seed_43/TD3_Hockey_mixed_seed_43_final.pth\",\n",
    "        # Add more paths as needed\n",
    "    ]\n",
    "\n",
    "    final_results = main(\n",
    "        mode=mode,\n",
    "        episodes=episodes,\n",
    "        seed=seed,\n",
    "        save_model=save_model,\n",
    "        training_config=custom_training_config,\n",
    "        load_agent_path=None,  # or \"models_hockey/vs_strong/seed_42/TD3_Hockey_vs_strong_seed_42_final.pth\" to resume\n",
    "        opponent_agent_paths=trained_opponent_paths  # Provide the list of paths to the trained agents\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_trained_agent(\n",
    "    agent_path, \n",
    "    opponent_types,  # List of opponent types to evaluate against\n",
    "    trained_opponent_paths=None,  # Dict mapping opponent_type to list of trained opponent paths\n",
    "    num_games=100, \n",
    "    env_mode=h_env.Mode.NORMAL,\n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluates the trained TD3 agent against specified opponents independently, ensuring side consistency.\n",
    "\n",
    "    Args:\n",
    "        agent_path (str): Path to the trained agent model file.\n",
    "        opponent_types (list of str): List of opponent types to evaluate against (\"weak\", \"strong\", \"shooting\", \"defense\", \"trained\").\n",
    "        trained_opponent_paths (dict of str: list of str or None): \n",
    "            - Keys are opponent types (e.g., \"trained\").\n",
    "            - Values are lists of paths to trained agent models for those types.\n",
    "            - For non-trained opponents, keys can be omitted or set to None.\n",
    "        num_games (int): Number of games to play for each evaluation per role.\n",
    "        env_mode (h_env.Mode): The environment mode to use for evaluation.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        dict: A nested dictionary containing statistics for each opponent type and side.\n",
    "    \"\"\"\n",
    "    # Initialize the environment\n",
    "    env = CustomHockeyEnv(mode=env_mode)\n",
    "    env.seed(seed)  # Parameterize seed if needed\n",
    "\n",
    "    # Set up directories for saving evaluation results\n",
    "    base_results_dir = \"./evaluation_results_hockey\"\n",
    "    os.makedirs(base_results_dir, exist_ok=True)\n",
    "\n",
    "    # Load the trained agent\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = 4  # Assuming action_dim is 4 as per training\n",
    "    max_action = float(env.action_space.high[0])\n",
    "\n",
    "    # Define a basic training configuration for initializing the agent\n",
    "    custom_training_config = {\n",
    "        \"discount\": 0.99,\n",
    "        \"tau\": 0.005,\n",
    "        \"policy_noise\": 0.2,\n",
    "        \"noise_clip\": 0.5,\n",
    "        \"policy_freq\": 2,\n",
    "        \"max_episodes\": 70000,  # Not used in evaluation\n",
    "        \"start_timesteps\": 1000,  # Not used in evaluation\n",
    "        \"eval_freq\": 200,  # Not used in evaluation\n",
    "        \"batch_size\": 256,\n",
    "        \"expl_noise_type\": \"pink\",\n",
    "        \"expl_noise\": 0.1,\n",
    "        \"pink_noise_params\": {\n",
    "            \"exponent\": 1.0,\n",
    "            \"fmin\": 0.0\n",
    "        },\n",
    "        \"ou_noise_params\": {\n",
    "            \"mu\": 0.0,\n",
    "            \"theta\": 0.15,\n",
    "            \"sigma\": 0.2\n",
    "        },\n",
    "        \"use_layer_norm\": True,\n",
    "        \"ln_eps\": 1e-5,\n",
    "        \"save_model\": False,  # Not saving during evaluation\n",
    "        \"save_model_freq\": 100,\n",
    "        \"use_rnd\": True,\n",
    "        \"rnd_weight\": 1.0,\n",
    "        \"rnd_lr\": 1e-4,\n",
    "        \"rnd_hidden_dim\": 128,\n",
    "        \"max_episode_steps\": 600\n",
    "    }\n",
    "\n",
    "    agent = TD3(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        max_action=max_action,\n",
    "        training_config=custom_training_config\n",
    "    )\n",
    "\n",
    "    agent.load(agent_path)\n",
    "    logging.info(f\"Loaded trained agent from {agent_path}\")\n",
    "\n",
    "    # Set networks to evaluation mode\n",
    "    agent.actor.eval()\n",
    "    agent.critic.eval()\n",
    "\n",
    "    # Load trained opponents if any\n",
    "    trained_opponents = {}\n",
    "    if trained_opponent_paths:\n",
    "        for opp_type, paths in trained_opponent_paths.items():\n",
    "            if opp_type not in opponent_types:\n",
    "                continue  # Only load trained opponents for specified types\n",
    "            trained_opponents[opp_type] = []\n",
    "            for path in paths:\n",
    "                trained_agent = TD3(\n",
    "                    state_dim=state_dim,\n",
    "                    action_dim=action_dim,\n",
    "                    max_action=max_action,\n",
    "                    training_config=custom_training_config\n",
    "                )\n",
    "                trained_agent.load(path)\n",
    "                trained_agent.actor.eval()\n",
    "                trained_agent.critic.eval()\n",
    "                trained_opponents[opp_type].append(trained_agent)\n",
    "                logging.info(f\"Loaded trained opponent agent from {path} for type '{opp_type}'\")\n",
    "                \n",
    "    print(trained_opponents)\n",
    "    # Initialize a dictionary to hold all statistics\n",
    "    all_stats = {}\n",
    "\n",
    "    # Iterate over each opponent type\n",
    "    for opponent_type in opponent_types:\n",
    "        logging.info(f\"\\n===== Evaluating against {opponent_type} opponent =====\")\n",
    "\n",
    "        # Setup logging for this opponent type\n",
    "        results_dir = os.path.join(base_results_dir, opponent_type)\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        setup_evaluation_logging(results_dir, seed, opponent_type)\n",
    "\n",
    "        # Initialize statistics containers\n",
    "        stats_agent1 = {'win': 0, 'loss': 0, 'draw': 0}\n",
    "\n",
    "        # Determine if the opponent type requires arbitrary actions\n",
    "        requires_arbitrary_actions = opponent_type in [\"shooting\", \"defense\"]\n",
    "\n",
    "        # Determine opponents based on type\n",
    "        if opponent_type == \"trained\":\n",
    "            # Use the list of trained opponents for this type\n",
    "            if opponent_type in trained_opponents and trained_opponents[opponent_type]:\n",
    "                opponents = trained_opponents[opponent_type]\n",
    "            else:\n",
    "                logging.warning(f\"No trained opponents provided for type '{opponent_type}'. Skipping evaluation.\")\n",
    "                continue\n",
    "        elif requires_arbitrary_actions:\n",
    "            # No opponent agent; actions will be arbitrary\n",
    "            opponents = [None]  # Placeholder\n",
    "        else:\n",
    "            # For built-in opponents, instantiate once\n",
    "            opponent = get_opponent(opponent_type, env)\n",
    "            opponents = [opponent]\n",
    "\n",
    "        # Evaluation Loop for Agent1 (Left Side)\n",
    "        for idx, current_opponent in enumerate(opponents):\n",
    "            # For 'shooting' and 'defense' modes, do not switch sides\n",
    "            is_switchable = False  # Always evaluate as Agent1\n",
    "\n",
    "            # If 'trained', evaluate separately for each trained opponent\n",
    "            if opponent_type == \"trained\":\n",
    "                opponent_identifier = f\"{opponent_type}_{idx + 1}\"\n",
    "            else:\n",
    "                opponent_identifier = opponent_type\n",
    "\n",
    "            logging.info(f\"\\n--- Evaluating against {opponent_identifier} ---\")\n",
    "\n",
    "            # Set the environment mode accordingly\n",
    "            if opponent_type == \"shooting\":\n",
    "                current_env_mode = h_env.Mode.TRAIN_SHOOTING\n",
    "            elif opponent_type == \"defense\":\n",
    "                current_env_mode = h_env.Mode.TRAIN_DEFENSE\n",
    "            else:\n",
    "                current_env_mode = h_env.Mode.NORMAL\n",
    "\n",
    "            # Reinitialize the environment with the appropriate mode\n",
    "            env.close()\n",
    "            env = CustomHockeyEnv(mode=current_env_mode)\n",
    "            env.seed(seed + idx)  # Different seed per opponent to ensure diversity\n",
    "\n",
    "            # Log the current environment mode\n",
    "            logging.info(f\"Environment mode set to: {current_env_mode}\")\n",
    "\n",
    "            # Initialize evaluation statistics for this opponent\n",
    "            stats_agent1_opponent = {'win': 0, 'loss': 0, 'draw': 0}\n",
    "\n",
    "            # Evaluate as Agent1 (Left Side)\n",
    "            logging.info(f\"Evaluating as Agent1 (Left Side) against {opponent_identifier}\")\n",
    "            for game in tqdm(range(1, num_games + 1), desc=f\"{opponent_identifier} - Agent1\"):\n",
    "                state, info = env.reset()\n",
    "                done = False\n",
    "\n",
    "                episode_reward1 = 0\n",
    "\n",
    "                while not done:\n",
    "                    # Agent1 selects action\n",
    "                    agent_action = agent.act(np.array(state), add_noise=False)\n",
    "\n",
    "                    # Opponent selects action\n",
    "                    if requires_arbitrary_actions:\n",
    "                        opponent_action = get_arbitrary_action(opponent_type)\n",
    "                    else:\n",
    "                        opponent_action = current_opponent.act(env.env.obs_agent_two())\n",
    "\n",
    "                    # Combine actions\n",
    "                    full_action = np.hstack([agent_action, opponent_action])\n",
    "\n",
    "                    # Step environment\n",
    "                    next_state, reward, done, info = env.step(full_action)\n",
    "                    episode_reward1 += reward\n",
    "\n",
    "                    # Prepare for next step\n",
    "                    state = next_state\n",
    "\n",
    "                # Determine the outcome\n",
    "                winner = info.get('winner', 0)  # 1 => agent1 wins, -1 => agent1 loses, 0 => draw\n",
    "\n",
    "                if winner == 1:\n",
    "                    stats_agent1_opponent['win'] += 1\n",
    "                elif winner == -1:\n",
    "                    stats_agent1_opponent['loss'] += 1\n",
    "                else:\n",
    "                    stats_agent1_opponent['draw'] += 1\n",
    "\n",
    "                # Log per-game results (optional)\n",
    "                logging.debug(f\"Game {game} as Agent1: Reward1={episode_reward1:.2f}, Winner={winner}\")\n",
    "\n",
    "            # Calculate statistics for Agent1\n",
    "            win_rate_agent1 = stats_agent1_opponent['win'] / num_games\n",
    "            loss_rate_agent1 = stats_agent1_opponent['loss'] / num_games\n",
    "            draw_rate_agent1 = stats_agent1_opponent['draw'] / num_games\n",
    "\n",
    "            stats = {}\n",
    "            stats[\"Agent1\"] = {\n",
    "                \"total_games\": num_games,\n",
    "                \"wins\": stats_agent1_opponent['win'],\n",
    "                \"losses\": stats_agent1_opponent['loss'],\n",
    "                \"draws\": stats_agent1_opponent['draw'],\n",
    "                \"win_rate\": win_rate_agent1,\n",
    "                \"loss_rate\": loss_rate_agent1,\n",
    "                \"draw_rate\": draw_rate_agent1,\n",
    "                \"mean_reward\": None,  # Placeholder if you decide to track rewards\n",
    "                \"std_reward\": None\n",
    "            }\n",
    "\n",
    "            # Aggregate statistics\n",
    "            all_stats[opponent_identifier] = stats\n",
    "\n",
    "            # Log the statistics\n",
    "            logging.info(\"\\n===== Evaluation Statistics =====\")\n",
    "            for side, side_stats in stats.items():\n",
    "                logging.info(f\"\\n{side} Statistics:\")\n",
    "                for key, value in side_stats.items():\n",
    "                    if value is not None:\n",
    "                        logging.info(f\"  {key}: {value:.4f}\")\n",
    "                    else:\n",
    "                        logging.info(f\"  {key}: N/A\")\n",
    "\n",
    "            # Plot overall statistics\n",
    "            plot_overall_statistics(\n",
    "                overall_stats=stats,\n",
    "                opponent_type=opponent_identifier,\n",
    "                save_path=results_dir\n",
    "            )\n",
    "\n",
    "            # Save detailed evaluation results\n",
    "            evaluation_results = {\n",
    "                \"Agent1\": {\n",
    "                    \"wins\": stats_agent1_opponent['win'],\n",
    "                    \"losses\": stats_agent1_opponent['loss'],\n",
    "                    \"draws\": stats_agent1_opponent['draw']\n",
    "                },\n",
    "                \"Statistics\": stats\n",
    "            }\n",
    "\n",
    "            with open(os.path.join(results_dir, f\"evaluation_results_{opponent_identifier}.json\"), \"w\") as f:\n",
    "                json.dump(evaluation_results, f, indent=4)\n",
    "\n",
    "    # After evaluating all opponent types, reset the environment to NORMAL mode\n",
    "    env.close()\n",
    "    env = CustomHockeyEnv(mode=h_env.Mode.NORMAL)\n",
    "    env.seed(seed)\n",
    "    logging.info(\"Environment reset to NORMAL mode.\")\n",
    "\n",
    "    # Save all statistics to a master JSON file\n",
    "    master_stats_path = os.path.join(base_results_dir, \"master_evaluation_stats.json\")\n",
    "    with open(master_stats_path, \"w\") as f:\n",
    "        json.dump(all_stats, f, indent=4)\n",
    "\n",
    "    logging.info(\"\\n===== All Evaluations Completed =====\")\n",
    "    return all_stats\n",
    "\n",
    "\n",
    "def get_arbitrary_action(opponent_type):\n",
    "    \"\"\"\n",
    "    Returns an arbitrary action based on the opponent type.\n",
    "    For 'shooting', returns a fixed shooting action.\n",
    "    For 'defense', returns a fixed defensive action.\n",
    "    \"\"\"\n",
    "    if opponent_type == \"shooting\":\n",
    "        return np.array([1, 0, 0, 1])  # Example shooting action\n",
    "    elif opponent_type == \"defense\":\n",
    "        return np.array([0.1, 0, 0, 1])  # Example defensive action\n",
    "    else:\n",
    "        # Default arbitrary action\n",
    "        return np.zeros(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'trained': [<TD3.td3.TD3 object at 0x7f60249308d0>, <TD3.td3.TD3 object at 0x7f6055715710>, <TD3.td3.TD3 object at 0x7f60244034d0>]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "strong - Agent1: 100%|██████████| 1000/1000 [00:22<00:00, 43.76it/s]\n",
      "weak - Agent1: 100%|██████████| 1000/1000 [00:21<00:00, 46.24it/s]\n",
      "shooting - Agent1: 100%|██████████| 1000/1000 [00:11<00:00, 90.72it/s]\n",
      "defense - Agent1: 100%|██████████| 1000/1000 [00:09<00:00, 102.79it/s]\n",
      "trained_1 - Agent1: 100%|██████████| 1000/1000 [00:33<00:00, 30.14it/s]\n",
      "trained_2 - Agent1: 100%|██████████| 1000/1000 [00:23<00:00, 43.42it/s]\n",
      "trained_3 - Agent1: 100%|██████████| 1000/1000 [00:48<00:00, 20.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Final Evaluation Statistics =====\n",
      "\n",
      "Opponent Identifier: strong\n",
      "  Agent1:\n",
      "    total_games: 1000.0000\n",
      "    wins: 977.0000\n",
      "    losses: 20.0000\n",
      "    draws: 3.0000\n",
      "    win_rate: 0.9770\n",
      "    loss_rate: 0.0200\n",
      "    draw_rate: 0.0030\n",
      "    mean_reward: N/A\n",
      "    std_reward: N/A\n",
      "\n",
      "Opponent Identifier: weak\n",
      "  Agent1:\n",
      "    total_games: 1000.0000\n",
      "    wins: 991.0000\n",
      "    losses: 6.0000\n",
      "    draws: 3.0000\n",
      "    win_rate: 0.9910\n",
      "    loss_rate: 0.0060\n",
      "    draw_rate: 0.0030\n",
      "    mean_reward: N/A\n",
      "    std_reward: N/A\n",
      "\n",
      "Opponent Identifier: shooting\n",
      "  Agent1:\n",
      "    total_games: 1000.0000\n",
      "    wins: 974.0000\n",
      "    losses: 11.0000\n",
      "    draws: 15.0000\n",
      "    win_rate: 0.9740\n",
      "    loss_rate: 0.0110\n",
      "    draw_rate: 0.0150\n",
      "    mean_reward: N/A\n",
      "    std_reward: N/A\n",
      "\n",
      "Opponent Identifier: defense\n",
      "  Agent1:\n",
      "    total_games: 1000.0000\n",
      "    wins: 966.0000\n",
      "    losses: 21.0000\n",
      "    draws: 13.0000\n",
      "    win_rate: 0.9660\n",
      "    loss_rate: 0.0210\n",
      "    draw_rate: 0.0130\n",
      "    mean_reward: N/A\n",
      "    std_reward: N/A\n",
      "\n",
      "Opponent Identifier: trained_1\n",
      "  Agent1:\n",
      "    total_games: 1000.0000\n",
      "    wins: 993.0000\n",
      "    losses: 6.0000\n",
      "    draws: 1.0000\n",
      "    win_rate: 0.9930\n",
      "    loss_rate: 0.0060\n",
      "    draw_rate: 0.0010\n",
      "    mean_reward: N/A\n",
      "    std_reward: N/A\n",
      "\n",
      "Opponent Identifier: trained_2\n",
      "  Agent1:\n",
      "    total_games: 1000.0000\n",
      "    wins: 992.0000\n",
      "    losses: 8.0000\n",
      "    draws: 0.0000\n",
      "    win_rate: 0.9920\n",
      "    loss_rate: 0.0080\n",
      "    draw_rate: 0.0000\n",
      "    mean_reward: N/A\n",
      "    std_reward: N/A\n",
      "\n",
      "Opponent Identifier: trained_3\n",
      "  Agent1:\n",
      "    total_games: 1000.0000\n",
      "    wins: 514.0000\n",
      "    losses: 481.0000\n",
      "    draws: 5.0000\n",
      "    win_rate: 0.5140\n",
      "    loss_rate: 0.4810\n",
      "    draw_rate: 0.0050\n",
      "    mean_reward: N/A\n",
      "    std_reward: N/A\n"
     ]
    }
   ],
   "source": [
    "agent_path = \"models_hockey/mixed/seed_44/TD3_Hockey_mixed_seed_44_final.pth\"\n",
    "opponent_types = [\"strong\", \"weak\", \"shooting\", \"defense\", \"trained\"]\n",
    "num_games = 1000  # Number of evaluation games\n",
    "env_mode = h_env.Mode.NORMAL  # Or other modes like TRAIN_SHOOTING, TRAIN_DEFENSE\n",
    "trained_opponent_agent_paths = {\n",
    "    \"trained\": [\n",
    "        \"models_hockey/vs_strong/seed_42/TD3_Hockey_vs_strong_seed_42_final.pth\",\n",
    "        \"models_hockey/mixed/seed_43/TD3_Hockey_mixed_seed_43_final.pth\",\n",
    "        \"models_hockey/mixed/seed_44/TD3_Hockey_mixed_seed_44_final.pth\"\n",
    "        # Add more paths as needed\n",
    "    ]\n",
    "}\n",
    "seed = 44\n",
    "\n",
    "evaluation_stats = evaluate_trained_agent(\n",
    "        agent_path=agent_path,\n",
    "        opponent_types=opponent_types,\n",
    "        trained_opponent_paths=trained_opponent_agent_paths,  # Provide the dict of trained opponent paths\n",
    "        num_games=num_games,\n",
    "        env_mode=env_mode,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "# Print the evaluation statistics\n",
    "print(\"\\n===== Final Evaluation Statistics =====\")\n",
    "for opponent, stats in evaluation_stats.items():\n",
    "    print(f\"\\nOpponent Identifier: {opponent}\")\n",
    "    for side, side_stats in stats.items():\n",
    "        print(f\"  {side}:\")\n",
    "        for key, value in side_stats.items():\n",
    "            if value is not None:\n",
    "                print(f\"    {key}: {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"    {key}: N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
