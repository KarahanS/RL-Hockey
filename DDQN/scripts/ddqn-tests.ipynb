{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDQN in Simpler Environments\n",
    "\n",
    "Base DQN implementation adapted from HW7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Adding the parent directory to the path to enable importing\n",
    "root_dir = os.path.dirname(os.path.abspath(\"../\"))\n",
    "if root_dir not in sys.path:\n",
    "    sys.path.append(root_dir)\n",
    "    \n",
    "from DDQN.DQN import DQNAgent, TargetDQNAgent, DoubleDQNAgent\n",
    "from DDQN.DDQN import DuelingDQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
    "\n",
    "\n",
    "class DiscreteActionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env: gym.Env, bins = 5):\n",
    "        \"\"\"A wrapper for converting a 1D continuous actions into discrete ones.\n",
    "        Args:\n",
    "            env: The environment to apply the wrapper\n",
    "            bins: number of discrete actions\n",
    "        \"\"\"\n",
    "        assert isinstance(env.action_space, spaces.Box)\n",
    "        super().__init__(env)\n",
    "        self.bins = bins\n",
    "        self.orig_action_space = env.action_space\n",
    "        self.action_space = spaces.Discrete(self.bins)\n",
    "\n",
    "    def action(self, action):\n",
    "        \"\"\" discrete actions from low to high in 'bins'\n",
    "        Args:\n",
    "            action: The discrete action\n",
    "        Returns:\n",
    "            continuous action\n",
    "        \"\"\"\n",
    "        return self.orig_action_space.low + action/(self.bins-1.0)*(self.orig_action_space.high-self.orig_action_space.low)  \n",
    "\n",
    "\n",
    "def create_env(env_name, render=False, discrete_wrapper=None):\n",
    "    if render:\n",
    "        env = gym.make(env_name, render_mode='human')\n",
    "    else:\n",
    "        env = gym.make(env_name)\n",
    "\n",
    "    if isinstance(env.action_space, spaces.Box):\n",
    "        env = (DiscreteActionWrapper(env, 5) if discrete_wrapper is None else discrete_wrapper)\n",
    "\n",
    "    ac_space = env.action_space\n",
    "    o_space = env.observation_space\n",
    "    print(\"Env. action space:\")\n",
    "    print(ac_space)\n",
    "    print(\"Env. observation space:\")\n",
    "    print(o_space)\n",
    "    #print(list(zip(env.observation_space.low, env.observation_space.high)))\n",
    "\n",
    "    return env, ac_space, o_space\n",
    "\n",
    "\n",
    "def train_dqn(q_agent, env, max_episodes=600, max_steps=500, print_freq=100):\n",
    "    stats = []\n",
    "    losses = []\n",
    "\n",
    "    for i in tqdm(range(max_episodes)):\n",
    "        # print(\"Starting a new episode\")\n",
    "        total_reward = 0\n",
    "        ob, _info = env.reset()\n",
    "        for t in range(max_steps):\n",
    "            done = False\n",
    "            a = q_agent.act(ob)\n",
    "            (ob_new, reward, done, trunc, _info) = env.step(a)\n",
    "            total_reward += reward\n",
    "            q_agent.store_transition((ob, a, reward, ob_new, done))            \n",
    "            ob=ob_new\n",
    "            if done: break\n",
    "        losses.extend(q_agent.train(32))\n",
    "        stats.append([i,total_reward,t+1])    \n",
    "        \n",
    "        if (i-1) % print_freq == 0:\n",
    "            print(\"{}: Done after {} steps. Reward: {}\".format(i, t+1, total_reward))\n",
    "\n",
    "    return stats, losses\n",
    "\n",
    "\n",
    "def plot_training(stats, losses):\n",
    "    stats_np = np.asarray(stats)\n",
    "    losses_np = np.asarray(losses)\n",
    "\n",
    "    fig=plt.figure(figsize=(6,3.8))\n",
    "    plt.plot(stats_np[:,1], label=\"return\")\n",
    "    plt.plot(running_mean(stats_np[:,1],20), label=\"smoothed-return\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(losses_np)\n",
    "\n",
    "\n",
    "def display_env(q_agent, env):\n",
    "    \"\"\"Display trained agent's performance. Human-mode rendering recommended for demonstration.\"\"\"\n",
    "\n",
    "    ob, _info = env.reset()\n",
    "    if isinstance(env.action_space, spaces.Box):\n",
    "        env = DiscreteActionWrapper(env,5)\n",
    "\n",
    "    env.reset()\n",
    "    total_reward = 0\n",
    "    for t in range(500):\n",
    "        a = q_agent.act(ob)\n",
    "        (ob, reward, done, trunc, _info) = env.step(a)\n",
    "        total_reward+= reward\n",
    "        if done or trunc: break\n",
    "\n",
    "    print(\"Total reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN\n",
    "Test DQN implementation on simple environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, ac_space, o_space = create_env(\"Pendulum-v1\")\n",
    "\n",
    "q_agent = DQNAgent(\n",
    "    o_space,\n",
    "    ac_space,\n",
    "    discount=0.95,\n",
    "    eps=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats, losses = train_dqn(q_agent, env, max_episodes=600, max_steps=500, print_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(stats, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display trained agent's performance\n",
    "env = gym.make(\"Pendulum-v1\", render_mode='human')\n",
    "\n",
    "display_env(q_agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LunarLander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, ac_space, o_space = create_env(\"LunarLander-v3\")\n",
    "\n",
    "q_agent = DQNAgent(\n",
    "    o_space,\n",
    "    ac_space,\n",
    "    discount=0.95,\n",
    "    eps=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats, losses = train_dqn(q_agent, env, max_episodes=1000, max_steps=600, print_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(stats, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display trained agent's performance\n",
    "env = gym.make(\"LunarLander-v3\", render_mode='human')\n",
    "\n",
    "display_env(q_agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN with Target Network\n",
    "\n",
    "Test Target-DQN implementation on simple environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, ac_space, o_space = create_env(\"Pendulum-v1\")\n",
    "\n",
    "q_agent = TargetDQNAgent(\n",
    "    o_space,\n",
    "    ac_space,\n",
    "    discount=0.95,\n",
    "    eps=0.2,\n",
    "    update_target_every=20,\n",
    "    tau=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats, losses = train_dqn(q_agent, env, max_episodes=600, max_steps=500, print_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(stats, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display trained agent's performance\n",
    "env = gym.make(\"Pendulum-v1\", render_mode='human')\n",
    "\n",
    "display_env(q_agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LunarLander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, ac_space, o_space = create_env(\"LunarLander-v3\")\n",
    "\n",
    "q_agent = TargetDQNAgent(\n",
    "    o_space,\n",
    "    ac_space,\n",
    "    discount=0.95,\n",
    "    eps=0.2,\n",
    "    update_target_every=20,\n",
    "    tau=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats, losses = train_dqn(q_agent, env, max_episodes=1000, max_steps=600, print_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(stats, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display trained agent's performance\n",
    "env = gym.make(\"LunarLander-v3\", render_mode='human')\n",
    "\n",
    "display_env(q_agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, ac_space, o_space = create_env(\"Pendulum-v1\")\n",
    "\n",
    "q_agent = DoubleDQNAgent(\n",
    "    o_space,\n",
    "    ac_space,\n",
    "    discount=0.95,\n",
    "    eps=0.2,\n",
    "    update_target_every=20,\n",
    "    tau=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats, losses = train_dqn(q_agent, env, max_episodes=600, max_steps=500, print_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(stats, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display trained agent's performance\n",
    "env = gym.make(\"Pendulum-v1\", render_mode='human')\n",
    "\n",
    "display_env(q_agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LunarLander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, ac_space, o_space = create_env(\"LunarLander-v3\")\n",
    "\n",
    "q_agent = DoubleDQNAgent(\n",
    "    o_space,\n",
    "    ac_space,\n",
    "    discount=0.95,\n",
    "    eps=0.2,\n",
    "    update_target_every=20,\n",
    "    tau=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats, losses = train_dqn(q_agent, env, max_episodes=1000, max_steps=600, print_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(stats, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display trained agent's performance\n",
    "env = gym.make(\"LunarLander-v3\", render_mode='human')\n",
    "\n",
    "display_env(q_agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dueling DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, ac_space, o_space = create_env(\"Pendulum-v1\")\n",
    "\n",
    "q_agent = DuelingDQNAgent(\n",
    "    o_space,\n",
    "    ac_space,\n",
    "    discount=0.95,\n",
    "    eps=0.2,\n",
    "    update_target_every=20,\n",
    "    tau=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats, losses = train_dqn(q_agent, env, max_episodes=600, max_steps=500, print_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(stats, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display trained agent's performance\n",
    "env = gym.make(\"Pendulum-v1\", render_mode='human')\n",
    "\n",
    "display_env(q_agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LunarLander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, ac_space, o_space = create_env(\"LunarLander-v3\")\n",
    "\n",
    "q_agent = DuelingDQNAgent(\n",
    "    o_space,\n",
    "    ac_space,\n",
    "    discount=0.95,\n",
    "    eps=0.2,\n",
    "    update_target_every=20,\n",
    "    tau=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats, losses = train_dqn(q_agent, env, max_episodes=1000, max_steps=600, print_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(stats, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display trained agent's performance\n",
    "env = gym.make(\"LunarLander-v3\", render_mode='human')\n",
    "\n",
    "display_env(q_agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
